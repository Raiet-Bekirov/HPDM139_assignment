{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb7b9c3-4af9-4609-a76a-60094232b2e5",
   "metadata": {},
   "source": [
    "# Single Group Fairness Metrics Demonstration\n",
    "![Fairness](assets/images/Fairness.jpg)\n",
    "<p align=\"center\">\n",
    "  <em>Figure 1: Fairness over Time</em><br>\n",
    "  <em>Source: TimeBots</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbcecbc-b8ea-4572-a4e2-0e582d0af487",
   "metadata": {},
   "source": [
    "# Group Fairness: Definition üêù\n",
    "Group Fairness in Machine Learning and AI refers to the idea that the performance of an algorithm should not disproportionately disadvantage or harm specific groups of people based on the sensitive attributes (e.g sex, age). \n",
    "\n",
    "## Distinction Between Group Fairness and Intersectional Fairness\n",
    "While this package emphasises the development of intersectional fairness metrics, single-group fairness metrics remain an essential component of any comprehensive fairness evaluation. Group fairness metrics assess model behaviour with respect to a single protected attribute (e.g., sex or age), providing an initial, high-level understanding of whether an algorithm performs equitably across broad demographic categories.\n",
    "\n",
    "In contrast, intersectional fairness metrics evaluate fairness across combinations of multiple protected attributes simultaneously. This evaluation enables the identification of ‚Äúcompounded‚Äù disadvantages experienced by individuals belonging to multiple marginalised groups - patterns that may remain hidden when attributes are analysed in isolation. \n",
    "\n",
    "Together, single-group and intersectional fairness metrics offer complementary perspectives: the former provides a general overview of fairness performance, while the latter facilitates a more granular and nuanced analysis of algorithmic bias.\n",
    "\n",
    "## Literature Review and Rationale for Metric Selection \n",
    "### Equalised Opportunity Difference (EOD)\n",
    "$$\n",
    "\\text{EOD} =\n",
    "\\text{TPR}_{\\text{unpriv}} - \\text{TPR}_{\\text{priv}}\n",
    "$$\n",
    "\n",
    "EOD is a group fairness metric proposed by Hardt et al. (2016) that measures the difference in True Positive Rates (TPR) between privileged and underprivileged groups. The metric is designed to assess whether a classifier provides equal opportunity for individuals who truly belong to the positive class, irrespective of their membership in a protected group.\n",
    "\n",
    "In the context of heart disease risk prediction, the positive outcome corresponds to correctly identifying individuals with heart disease, rather than the presence of a positive model prediction itself. An ideal EOD value of zero indicates \"fairness\", whereas a score with larger magnitude indicate increasing levels of unfairness. The sign of the metric conveys the direction of disparity, with positive values indicating that the privileged group benefits from higher true positive rates, and negative values indicating disadvantage for the privileged group.\n",
    "\n",
    "### Average Odds Difference (AOD)\n",
    "$$\n",
    "\\text{AOD} = \\frac{1}{2} \\Big[\n",
    "(\\text{TPR}_{\\text{unpriv}} - \\text{TPR}_{\\text{priv}})\n",
    "+\n",
    "(\\text{FPR}_{\\text{unpriv}} - \\text{FPR}_{\\text{priv}})\n",
    "\\Big]\n",
    "$$\n",
    "AOD is a group fairness metric that emerged as a response to limitations identified in earlier fairness definitions, including those proposed by Hardt et al. (2016). Rather than being introduced as a standalone theoretical framework, AOD is commonly discussed in the literature as a practical extension of Equalised Odds.\n",
    "\n",
    "Previous work, Statistical Parity Difference (SPD) considers only disparities in positive prediction rates, and Equalised Opportunity Difference (EOD) focuses exclusively on differences in true positive rates; AOD incorporates information from both true positive rates and false positive rates.\n",
    "\n",
    "This metric works similar as the former, where a magnitude of 0 and 1 indicate fairness and unfairness respectively. \n",
    "\n",
    "### Disparate Impact (DI)\n",
    "$$\n",
    "\\text{DI} =\n",
    "\\frac{P(\\hat{Y} = 1 \\mid A = \\text{unpriv})}\n",
    "     {P(\\hat{Y} = 1 \\mid A = \\text{priv})}\n",
    "$$\n",
    "Disparate Impact (DI) is a metric in fairness quanitfying the proportion of groups receiving positive outcomes. A value of 0 indicates fairness, with over 0 being biased towards the privileged group and a value under 0 means that it favours the underprivileged group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e1bf5-2db1-479e-8e4b-52bf3a0d5b5d",
   "metadata": {},
   "source": [
    "## Code Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008a54c-e9e1-4328-94d1-02a168fbdec2",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ed741-18c1-493c-b651-fab5e1cd18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36cbab-7209-4114-9bc6-fb41f15a34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from fairness.data import load_heart_csv\n",
    "from fairness.preprocess import add_age_group, map_binary_column, \\\n",
    "                                apply_transforms, preprocess_tabular, \\\n",
    "                                make_train_test_split\n",
    "from fairness.groups import make_eval_df\n",
    "from fairness.single_metrics import calculate_EOD\n",
    "from fairness.single_metrics import calculate_AOD\n",
    "from fairness.single_metrics import calculate_DI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fe3a8-9c08-4a74-8f47-e7b61634214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent if Path.cwd().name == \"examples\" else Path.cwd()\n",
    "DATA_PATH = ROOT / \"data\" / \"heart.csv\"\n",
    "\n",
    "df_raw = load_heart_csv(DATA_PATH)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ec300-0f11-44f2-832b-8f8174e3ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a protected attribute for fairness analysis \n",
    "df_fair = add_age_group(df_raw, age_col=\"Age\", new_col=\"age_group\", bins=(0, 55, 120), labels=(\"young\", \"older\"))\n",
    "\n",
    "# map binary/categorical encodings if needed (if dataset has M/F)\n",
    "if \"Sex\" in df_fair.columns and df_fair[\"Sex\"].dtype == object:\n",
    "    df_fair = map_binary_column(df_fair, col=\"Sex\", mapping={\"M\": 1, \"F\": 0})\n",
    "\n",
    "df_fair[[\"Age\", \"age_group\", \"Sex\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693961ea-28d1-4019-942a-af43d6465ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fair2 = apply_transforms(\n",
    "    df_raw,\n",
    "    transforms=[\n",
    "        lambda d: add_age_group(d, age_col=\"Age\", new_col=\"age_group\"),\n",
    "        lambda d: map_binary_column(d, col=\"Sex\", mapping={\"M\": 1, \"F\": 0}),\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_fair2[[\"Age\", \"age_group\",\"Sex\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1c48f-b1be-4f5d-b11e-a19b3210f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = preprocess_tabular(df_fair, drop_cols=(\"age_group\",))\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55441772-ec1c-4b7e-9e3d-4aa2cde5bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = make_train_test_split(\n",
    "    df_model,\n",
    "    target_col=\"HeartDisease\",\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=True,\n",
    ")\n",
    "\n",
    "split.X_train.shape, split.X_test.shape, split.y_train.shape, split.y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd747e1-a587-45ea-98bd-9de60af7eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "model.fit(split.X_train, split.y_train)\n",
    "y_pred = model.predict(split.X_test)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(split.y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffcb52c-ad48-448f-9eb6-822db1b84900",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_fair.loc[split.X_test.index]\n",
    "\n",
    "eval_df = make_eval_df(\n",
    "    df_test=df_test,\n",
    "    protected=[\"Sex\"],\n",
    "    y_pred=y_pred,\n",
    "    y_true=split.y_test.to_numpy(),\n",
    ")\n",
    "\n",
    "eval_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41a073-d1e1-42c3-be7e-9bf8131bcf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.adapters import unpack_eval_df, make_subject_labels_dict\n",
    "\n",
    "subject_labels, predictions, true_statuses = unpack_eval_df(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27bfa4a-60b8-4a45-8c07-04cbddfdad61",
   "metadata": {},
   "source": [
    "## ‚≠ê Demonstration for the three single metrics: EOD, AOD and DI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483bc210-5ffb-4b2b-b890-280a7e825d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairness.single_metrics as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf4edf8-3d17-47f9-9f3d-97becf1ef004",
   "metadata": {},
   "source": [
    "### Equalised Opportunity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582aa89b-23eb-4be2-a653-ccc73bd5c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.calculate_EOD(\n",
    "    y_test=true_statuses,\n",
    "    y_pred=predictions,\n",
    "    group_labels=subject_labels,\n",
    "    privileged_label='Sex=0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c951c64-9f45-4689-ac27-ee5244d1fe93",
   "metadata": {},
   "source": [
    "### Average Odds Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b70c13-b564-4547-946e-83ac7d9e2abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.calculate_AOD(\n",
    "    y_test=true_statuses,\n",
    "    y_pred=predictions,\n",
    "    group_labels=subject_labels,\n",
    "    privileged_label='Sex=0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba217e-d38e-4819-aa3b-de4fbe84d4ef",
   "metadata": {},
   "source": [
    "### Disparate Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbfba5c-bd8d-4b42-8bc4-9f087d156843",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.calculate_DI(\n",
    "    y_pred=predictions,\n",
    "    group_labels=subject_labels,\n",
    "    privileged_label='Sex=1'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
