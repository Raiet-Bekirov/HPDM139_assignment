{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d7a20a",
   "metadata": {},
   "source": [
    "# Intersectional Fairness: End-to-end demonstration\n",
    "\n",
    "This notebook demonstrates how to use the data utilities provided by the package to prepare a health dataset and perform intersectional fairness analysis functions, including visualisations. \n",
    "\n",
    "> **Note:**\n",
    "> Model training is included in this notebook only to generate predictions (y_pred) for demonstration purposes.\n",
    "> The data loading, preprocessing, and grouping modules are model-agnostic and can be used with any classifier that produces predictions aligned to the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.data import load_heart_csv\n",
    "from fairness.preprocess import add_age_group, map_binary_column, \\\n",
    "                                apply_transforms, preprocess_tabular, \\\n",
    "                                make_train_test_split\n",
    "from fairness.groups import make_eval_df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834739d5",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "This demo uses the `heart.csv` file. \n",
    "\n",
    "Heart failure is a common event caused by cardiovascular diseases, and this dataset contains 11 features that can be used to predict possible heart failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent if Path.cwd().name == \"examples\" else Path.cwd()\n",
    "DATA_PATH = ROOT / \"data\" / \"heart.csv\"\n",
    "\n",
    "df_raw = load_heart_csv(DATA_PATH)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c72364",
   "metadata": {},
   "source": [
    "## Fairness-oriented preprocessing\n",
    "\n",
    "Continuous protected attributes (like age) are binned into a small number of categories\n",
    "to produce interpretable groups and avoid tiny subgroup sample sizes.\n",
    "\n",
    "There is an optional mapping for a binary protected attribute (e.g. `Sex` from `\"M\"/\"F\"` to `1/0`),\n",
    "depending on how the dataset encodes it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56512a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a protected attribute for fairness analysis \n",
    "df_fair = add_age_group(df_raw, age_col=\"Age\", new_col=\"age_group\",\n",
    "                        bins=(0, 55, 120), labels=(\"young\", \"older\"))\n",
    "\n",
    "# map binary/categorical encodings if needed (if dataset has M/F)\n",
    "if \"Sex\" in df_fair.columns and df_fair[\"Sex\"].dtype == object:\n",
    "    df_fair = map_binary_column(df_fair, col=\"Sex\", mapping={\"M\": 1, \"F\": 0})\n",
    "\n",
    "df_fair[[\"Age\", \"age_group\", \"Sex\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab4c30",
   "metadata": {},
   "source": [
    "### Using `apply_transforms`\n",
    "\n",
    "`apply_transforms` allows multiple `DataFrame -> DataFrame` operations to be chained togther. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fair2 = apply_transforms(\n",
    "    df_raw,\n",
    "    transforms=[\n",
    "        lambda d: add_age_group(d, age_col=\"Age\", new_col=\"age_group\"),\n",
    "        lambda d: map_binary_column(d, col=\"Sex\", mapping={\"M\": 1, \"F\": 0}),\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_fair2[[\"Age\", \"age_group\",\"Sex\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92865a93",
   "metadata": {},
   "source": [
    "## Model-oriented preprocessing\n",
    "\n",
    "In the raw dataset, variables are represented using a mixture of numeric and categorical encodings, reflecting how the data were originally defined and collected.\n",
    "\n",
    "Binary clinical indicators such as `FastingBS (0, 1)` are passed through unchanged.\n",
    "\n",
    "Variables that represent categorical concepts with 2 or ,ore possible values, such as `ChestPainType (TA, ATA, NAP, ASY)` are converted into nuermic features using one-hot encoding. This creates binary indicator columns that take value `True` if the category applies to the individual, else `False`.\n",
    "\n",
    "This allows interpretation by machine learning models.\n",
    "\n",
    "Some protected characteristics, such as sex, may be clinically relevant predictors\n",
    "and are therefore retained in the model inputs. Derived protected attributes used\n",
    "only for fairness analysis (e.g. `age_group`) are excluded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = preprocess_tabular(df_fair, drop_cols=(\"age_group\",))\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2819c726",
   "metadata": {},
   "source": [
    "## Train/test split \n",
    "\n",
    "`make_train_test_split` returns a `SplitData` container:\n",
    "- `X_train`, `X_test`\n",
    "- `y_train`, `y_test`\n",
    "\n",
    "Derived protected attributes (e.g. `age_group`) are dropped from the model features for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43261e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = make_train_test_split(\n",
    "    df_model,\n",
    "    target_col=\"HeartDisease\",\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=True,\n",
    ")\n",
    "\n",
    "split.X_train.shape, \\\n",
    "split.X_test.shape, \\\n",
    "split.y_train.shape, \\\n",
    "split.y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a4772",
   "metadata": {},
   "source": [
    "## Train a model to generate `y_pred`\n",
    "\n",
    "This step is outside of the toolkit's data loading and processing modules.\n",
    "It is included here to show how `y_pred` can be \n",
    "produced for fairness metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc329e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "model.fit(split.X_train, split.y_train)\n",
    "y_pred = model.predict(split.X_test)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(split.y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d2989",
   "metadata": {},
   "source": [
    "The table abvove simmarises the predictive performance of the trained classifier on the test set, using standard classification metrics.\n",
    "\n",
    "Each row corresponds to one outcome class:\n",
    "- Class 0: no heart disease\n",
    "- Class 1: heart disease present\n",
    "\n",
    "\n",
    "Accuracy is the proportion of all test-set predictions that are correct An accuracy of 0.88 means the model predicts the correct outcome for 88% of patients in the test set. In this example, the classifier achieves good accuracy, providing a suitable baseline for fairness analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17fc37",
   "metadata": {},
   "source": [
    "## Make an evaulation DataFrame\n",
    "\n",
    "The `eval_df` DataFrame brings together all information required for fairness analysis in a single, aligned table.\n",
    "\n",
    "A list of protected attributes is provided to define intersectional groups (for example, sex and age group).\n",
    "\n",
    "Each row corresponds to one individual in the test set, with the following columns:\n",
    "\n",
    "- `subject_label`, the individual’s intersectional protected group\n",
    "- `y_pred`, the model’s prediction\n",
    "- `y_true`, the true outcome\n",
    "\n",
    "Because these columns are aligned row-by-row, fairness metrics can safely compare model performance across protected and intersectional groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d48f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_fair.loc[split.X_test.index]\n",
    "\n",
    "eval_df = make_eval_df(\n",
    "    df_test=df_test,\n",
    "    protected=[\"Sex\",\"age_group\",],\n",
    "    y_pred=y_pred,\n",
    "    y_true=split.y_test.to_numpy(),\n",
    ")\n",
    "\n",
    "eval_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd08545",
   "metadata": {},
   "source": [
    "## Using the evaluation DataFrame with fairness metrics\n",
    "\n",
    "Different fairness metrics expect their inputs in slightly different formats.\n",
    "\n",
    "Many fairness metrics operate on lists, for example:\n",
    "\n",
    "- a list of group labels\n",
    "- a list of predictions\n",
    "- a list of true outcomes\n",
    "\n",
    "These can be extracted from `eval_df` using helper functions in `adapters.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cbc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.adapters import unpack_eval_df, make_subject_labels_dict\n",
    "\n",
    "subject_labels, predictions, true_statuses = unpack_eval_df(eval_df)\n",
    "\n",
    "subject_labels_dict = make_subject_labels_dict(\n",
    "    df_test, protected_cols=[\"Sex\", \"age_group\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910bc2c4",
   "metadata": {},
   "source": [
    "## Example 1: Intersectional Accuracy Differences\n",
    "\n",
    "This example computes the natural log of the maximum accuracy ratio across all intersectional groups defined by the protected attributes sex and age_group.\n",
    "\n",
    "A value close to 0 indicates that model accuracy is similar across all intersectional groups.\n",
    "\n",
    "Larger values indicate greater disparity, meaning some groups benefit from much higher accuracy than others.\n",
    "\n",
    "In a healthcare context, a large maximum intersectional accuracy difference suggests that the model may perform substantially better for certain demographic groups, raising concerns about equity and potential harm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.metrics import all_intersect_accs, max_intersect_acc_ratio\n",
    "\n",
    "all_accs = all_intersect_accs(subject_labels_dict,\n",
    "                              predictions,\n",
    "                              true_statuses)\n",
    "print(\"Model accuracy in different groups:\", all_accs)\n",
    "\n",
    "maxrat = max_intersect_acc_ratio(subject_labels_dict,\n",
    "                                 predictions,\n",
    "                                 true_statuses,\n",
    "                                 natural_log=False)\n",
    "print(\"Maximum intersectional accuaracy ratio:\", maxrat)\n",
    "\n",
    "maxrat_log = max_intersect_acc_ratio(subject_labels_dict,\n",
    "                                     predictions,\n",
    "                                     true_statuses,\n",
    "                                     natural_log=True)\n",
    "print(\"Natural log of maximum intersectional accuaracy ratio:\", maxrat_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f26e46",
   "metadata": {},
   "source": [
    "## Example 2: False Omission Rates\n",
    "\n",
    "The False Omission Rate (FOR) for a specific intersectional group measures the risk of false reassurance after a negative prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.metrics import intersect_for\n",
    "\n",
    "# define the specific intersectional group we want\n",
    "group_labels_dict = {\"Sex\": 1, \"age_group\": \"older\"}\n",
    "\n",
    "for_older_male = intersect_for(\n",
    "    group_labels_dict=group_labels_dict,\n",
    "    subject_labels_dict=subject_labels_dict,\n",
    "    predictions=predictions,\n",
    "    true_statuses=true_statuses,\n",
    ")\n",
    "\n",
    "print(\"False Omission Rate (older males):\", for_older_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4695cf1e-59ec-464a-822f-defef0c5eb21",
   "metadata": {},
   "source": [
    "Now let's look at the false omission rates in all groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd748ae-9bf2-4ff8-a7a3-d98041a332d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.metrics import all_intersect_fors\n",
    "\n",
    "all_fors = all_intersect_fors(subject_labels_dict, predictions, true_statuses)\n",
    "print(\"False omission rates in different groups:\", all_fors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a449931-8214-47c6-bd26-441345f80fd1",
   "metadata": {},
   "source": [
    "Since one of the false omission rates is zero, the function `max_intersect_acc_ratio` returns a NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185382ca-c6cc-4576-a387-fd4db92eb4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.metrics import max_intersect_for_ratio\n",
    "\n",
    "maxrat = max_intersect_for_ratio(subject_labels_dict,\n",
    "                                 predictions,\n",
    "                                 true_statuses,\n",
    "                                 natural_log=False)\n",
    "print(\"Maximum intersectional accuaracy ratio:\", maxrat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38daff0-b8bd-4ac0-8cbc-db271f962275",
   "metadata": {},
   "source": [
    "## Example 3: False Negative Rates\n",
    "\n",
    "The False Negative Rate (FNR) for a specific intersectional group measures the risk of being given a person being assigned a negative prediction given that they do in fact have the disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99fc6c9-84e6-4716-8592-80053a729636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.metrics import intersect_fnr\n",
    "\n",
    "# define the specific intersectional group we want\n",
    "group_labels_dict = {\"Sex\": 0, \"age_group\": \"young\"}\n",
    "\n",
    "fnr_young_female = intersect_fnr(\n",
    "    group_labels_dict=group_labels_dict,\n",
    "    subject_labels_dict=subject_labels_dict,\n",
    "    predictions=predictions,\n",
    "    true_statuses=true_statuses,\n",
    ")\n",
    "\n",
    "print(\"False Omission Rate (young females):\", fnr_young_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7cba0-1a3f-4f7e-87e9-c724503f41d0",
   "metadata": {},
   "source": [
    "We can use the functions `all_intersect_fnrs` and `max_intersect_fnr_diff` to find the FNR for each intersectional group and the maximum difference between their rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d8c0d-b52e-4432-bc94-462419062771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.metrics import all_intersect_fnrs, max_intersect_fnr_diff\n",
    "\n",
    "all_fnrs = all_intersect_fnrs(subject_labels_dict,\n",
    "                              predictions,\n",
    "                              true_statuses)\n",
    "print(\"False negative rates in different groups:\", all_fnrs)\n",
    "\n",
    "maxdiff = max_intersect_fnr_diff(subject_labels_dict,\n",
    "                                 predictions,\n",
    "                                 true_statuses)\n",
    "print(\"Maximum intersectional FNR difference:\", maxdiff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
